<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Siraj Rawood (RWDMOH001)">

<title>A Quarto Website - STA5073Z Assignment 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">A Quarto Website</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./RWDMOH001.html" rel="" target="" aria-current="page">
 <span class="menu-text">Home</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis" class="nav-link" data-scroll-target="#exploratory-data-analysis">Exploratory Data Analysis</a>
  <ul class="collapse">
  <li><a href="#bag-of-words-and-tf-idf" id="toc-bag-of-words-and-tf-idf" class="nav-link" data-scroll-target="#bag-of-words-and-tf-idf">Bag-of-Words and TF-IDF</a></li>
  <li><a href="#imbalanced-data" id="toc-imbalanced-data" class="nav-link" data-scroll-target="#imbalanced-data">Imbalanced Data</a></li>
  </ul></li>
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method">Method</a>
  <ul class="collapse">
  <li><a href="#data-splitting" id="toc-data-splitting" class="nav-link" data-scroll-target="#data-splitting">Data Splitting</a></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks">Neural Networks</a></li>
  <li><a href="#multinomial-logistic-regression" id="toc-multinomial-logistic-regression" class="nav-link" data-scroll-target="#multinomial-logistic-regression">Multinomial Logistic Regression</a></li>
  <li><a href="#decision-trees" id="toc-decision-trees" class="nav-link" data-scroll-target="#decision-trees">Decision Trees</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">STA5073Z Assignment 1</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Siraj Rawood (RWDMOH001) </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Department of Statistical Sciences, University of Cape Town
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  

</header>

<div style="page-break-after: always;"></div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>For this assignment we will concentrate on working with text data and using different methods to clean the data, making it usable for predictive modelling - given a sentence of text, which South African president was more likely to have said it.</p>
<p>To start the analysis we first load the packages needed as well as the data with an introduction to the dataset. Then we can begin data cleaning and preprocessing, leading to the initial exploration of the dataset. The aim at this point is to understand the dataset to better guide the analysis in later stages and inform the partitioning of the data into training and testing sets. Subsequently, we transition to transforming the data into a bag-of-words model, including further text processing steps such as eliminating stop words and using TF-IDF weighted counts. The next step involves the introduction of neural networks, multinomial logistic regression and decision trees, with a high-level background and outline our modeling approach. We address issues related to data imbalance, splitting training and testing data and methods for assessing model performance. Lastly, a brief discussion of the results and an overall conclusion.</p>
</section>
<section id="exploratory-data-analysis" class="level1">
<h1>Exploratory Data Analysis</h1>
<p>The data is contained within the <code>sona-addresses-1994-2023.zip</code> file. It contains text files for each of the State of the Nation Address (SONA) speeches from 1994 to 2023 given by the South African president of the corresponding year. This is an annual even in which provides an opportunity for the country’s president to address the nation, its legislative body (e.g., parliament or congress), and the broader public on the status of the country and priorities for the future.The data can be sourced from the <a href="https://www.gov.za/state-nation-address">SONA website</a>. Since this event happens twice during an election year, the file comprises of 36 speeches.</p>
<p>We make use of the <code>readtext()</code> function from the <em>readtext</em> package to read in all the <code>.txt</code> files. This is then converted to a tibble to make it comply with the data types needed for cleaning using <em>dplyr</em>.</p>
<p>We first reformat the date, convert all text to lower case and create a new column to show which president gave the respective speech called <em>president</em> - this becomes our response variable. We turn the data into tidy format and tokenise it - a unit of text to use for analysis: in this case into words and sentences. The <code>unnest_tokens()</code> function splits the text into a new data frame with one token per row, making the data tidy. We further clean the data by removing the dates and certain punctuation and escape sequences. Lastly we also remove stop words.</p>
<p>For the exploration of the data we will look for any patterns emerging from the data, see what the presidents say most often and the language they use.</p>
<p>The plot below shows us the most common words used by each president:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="RWDMOH001_files/figure-html/word exploring-1.png" class="img-fluid figure-img" width="960"></p>
</figure>
</div>
</div>
</div>
<p>From this we can see that there are certain words that are mentioned more often by certain presidents. De Klerk only has one speech from 1994 but we can see he talks a lot about the transformation South Africa was soon to face such as “constitution” and “freedom”. The same with Mandela in that he mentions “government”, “public” and “people” many times - how he has to ensure the new government can serve the country and its people.</p>
<p>All presidents also use the words “south” and “africa”/“african” very often. This is understandable as they need to use the demonym for the country. It would also suggest that these words won’t have much of an influence on the models. This also shows that there are no clear differences between the words used by each president and not much to distinguish between them.</p>
<p>To allow us to see differences more clearly, we will use term frequency – inverse document frequency (TF-IDF). This will weight words used across the speeches and place more emphasis on the words that are less common between the different presidents. Words with higher weights show greater value between the presidents, showing which may have higher predictive value. The plot below shows this:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="RWDMOH001_files/figure-html/tfidf in r-1.png" class="img-fluid figure-img" width="960"></p>
</figure>
</div>
</div>
</div>
<p>From this plot we can see the words seperating the presidents more clearly. For instance it can be seen that Ramaphosa mentions “covid” and the “pandemic”. This holds a high score as no other president encountered this in their tenure and has said this, however the rest of the words are scored very low. De Klerk has high scores for a few words, showing that he uses these more often than other presidents and is clearly distinguished from the rest. Whereas Zuma places high emphasis on greetings, he also shares that with Mbeki and his other highest scored words are still relatively low. We will be able to see how this analysis translates into the models. It can be seen that south and africa are not in these graphs due to it now having a low score - it has a high frequency amongst all presidents.</p>
<section id="bag-of-words-and-tf-idf" class="level2">
<h2 class="anchored" data-anchor-id="bag-of-words-and-tf-idf">Bag-of-Words and TF-IDF</h2>
<p>The tokenised data is now put into a bag of words format - this shows the occurrence of words within a document. This ignores grammar and focuses on the frequency counts of each word, and in this case, each word per sentence. These word frequencies are then used as the features in the subsequent models. Based on the frequency of words within a particular sentence, which president said it in their speech.</p>
<p>To ensure that the sentence structure is maintained, each sentence from a president is a row of data. For each row it is tokenised for each word in the sentence and count its frequency. Note that, for this problem, we shall look at the frequency of the 500 most popular words. Each sentence is represented by a row and each word is represented by a column. The frequency of that word in each sentence is given as the elements. The first column (<em>index</em>) indicates the sentence index. The second column (<em>pres_name</em>) shows the president from which the sentence is taken from. Besides the data described above, we also generate a bag-of-words model using the TF-IDF method. It was done to provide the previous figure however we shall do it more formally and create a usable dataset.</p>
</section>
<section id="imbalanced-data" class="level2">
<h2 class="anchored" data-anchor-id="imbalanced-data">Imbalanced Data</h2>
<p>What can also be seen from the exploration is that the data is ‘imbalanced’. This refers to the classes not being represented equally. Given our data we have the following number of sentences for each president:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>De Klerk</th>
<th>Mandela</th>
<th>Mbeki</th>
<th>Motlanthe</th>
<th>Ramaphosa</th>
<th>Zuma</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Count</td>
<td>91</td>
<td>1578</td>
<td>2337</td>
<td>249</td>
<td>2201</td>
<td>2486</td>
</tr>
</tbody>
</table>
<p>De Klerk amd Motlanthe have less than 100 sentences, Mandela over 1000, and Mbeki, Zuma and Ramaphosa have over 2000 sentences where ideally we would like them to have a similar amount. This shows the imbalance across the classes. Usually a small difference often does not matter, however, given the imbalance we have, it is not likely that we will have good classifiers.</p>
<p>Our models may fall in the <em>accuracy</em> paradox where accuracy measures may be good however it mimics the underlying distribution of the classes - it predicts everything for the majority classes/does not try to predict for the under-represnted classes, giving a high accuracy. Therefore it is also important to consider other measures of accuracy when assessing classification models.</p>
<p>To fix our imbalanced data problem we can look at under-sampling - removing observations from the bigger classes or over-sampling methods, adding observations to the smaller classes. Ideally under-sampling wouldn’t be the best choice since we are cutting down on the data we have which is already not much. Therefore an oversampling method we choose is Random Over-Sampling Examples (ROSE). Using the <em>ROSE</em> package in R, and the function <code>ovun.sample()</code> we do both oversampling and under-sampling. We have choose arbitrarily to have around 1000 observations for each president (sentences for each president).</p>
<p>After implementing the ROSE method, we now have the following distribution of the classes</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>De Klerk</th>
<th>Mandela</th>
<th>Mbeki</th>
<th>Motlanthe</th>
<th>Ramaphosa</th>
<th>Zuma</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Count</td>
<td>963</td>
<td>974</td>
<td>967</td>
<td>969</td>
<td>998</td>
<td>1037</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="method" class="level1">
<h1>Method</h1>
<section id="data-splitting" class="level2">
<h2 class="anchored" data-anchor-id="data-splitting">Data Splitting</h2>
<p>To create our models we need to split the data into training, test/validation and holdout sets. We will use a 70/30 split for training and holdout respectively, with an 80/20 split within the training and test/validation set when training the neural networks.</p>
</section>
<section id="neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks">Neural Networks</h2>
<p>A feedforward neural network is a fundamental architecture in neural networks. It comprises of an input layer, one or more hidden layers, and an output layer. The input layer receives the initial data or features, with each neuron representing a distinct feature. The network’s computation takes place in the hidden layers, where each neuron calculates a weighted sum of its inputs from the previous layer and applies an activation function. These weighted connections are learnable parameters of the network and are adjusted during training to minimize the difference between the network’s predictions and the true target values. The output layer generates the final output, and the choice of activation function here depends on the specific task, such as softmax for classification.</p>
<p>During forward propagation, data flows through the network, with each layer processing and passing information to the next layer. The output of the output layer represents the network’s final prediction, whether it’s a classification, regression value, or any other relevant output. To train the network, a loss function is used to measure the difference between the network’s predictions and the actual target values. The network then uses an optimization algorithm, often gradient descent, to update the weights through a process called backpropagation. Training involves iteratively adjusting these weights on a large dataset through multiple epochs until the network’s performance reaches a satisfactory level. The feedforward neural network learns to make predictions or classifications by processing data in a sequential manner and adapting its weights through training to improve its accuracy for a given task.</p>
<p>For the assignment we are using a sentence as an input and the output being the president who said it - this will be the prediction. We will create a model for the bag-of-words data, the TF-IDF data as well as for the ROSE data. The model is a simple neural network with 10 units in input layer, with ReLu activation function, 8 units in hidden layer with ReLu activation function and 6 units in output layer with softmax function using the the Adam optimiser with a learning rate of 0.01. We train this over a batch size of 5 for a total of 40 epochs.</p>
</section>
<section id="multinomial-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="multinomial-logistic-regression">Multinomial Logistic Regression</h2>
<p>Multinomial logistic regression is a statistical method for predicting the probabilities of multiple classes in a categorical variable. It extends binary logistic regression to handle more than two outcomes. The model uses independent variables to estimate parameters through maximum likelihood, and a softmax function converts raw predictions into probabilities, ensuring they sum to 1 for all classes. This approach is commonly employed in machine learning for tasks involving classification with multiple possible outcomes.</p>
</section>
<section id="decision-trees" class="level2">
<h2 class="anchored" data-anchor-id="decision-trees">Decision Trees</h2>
<p>Decision trees are a machine learning algorithm used for both classification and regression tasks. They work by recursively partitioning the data based on the values of features, creating a tree-like structure. Each internal node represents a decision based on a specific feature, and each leaf node represents the predicted outcome. The algorithm makes decisions by evaluating features at each node and branching accordingly. Decision trees are interpretable, easy to understand, and can handle both numerical and categorical data</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>To interpret our results we look at the confusion matrix of the outputs. This provides us with different metrics other than accuracy. We will only be concerned with 3 of these. As mentioned, we will assess the accuracy, as well as the p-values and kappa values. The accuracy is calculated as the percentage of the predictions that were correctly classified, the higher the value is, the better the model is at predicting the correct classes. The p-values show which models are significant at different levels, inversely, the lower the value, the greater the significance of the model. Lastly the Kappa value, this shows the level of agreement between predicted and actual classifications in a confusion matrix. A Kappa score of 1 indicates perfect agreement, 0 suggests agreement equivalent to chance, and -1 indicates complete disagreement. It is particularly valuable for evaluating classification models on imbalanced datasets. The formula incorporates observed and expected agreement, offering insights into a model’s performance beyond simple accuracy metrics. Hence why it is of importance when asessing models for this data set.</p>
<p>The results from the models are summarised below:</p>
<table class="table">
<colgroup>
<col style="width: 38%">
<col style="width: 23%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Dataset</th>
<th>Accuracy</th>
<th>P-Value</th>
<th>Kappa Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Neural Network</td>
<td>Bag of words</td>
<td>0.3006</td>
<td>0.009159</td>
<td>0.058</td>
</tr>
<tr class="even">
<td></td>
<td>TF-IDF</td>
<td>0.3002</td>
<td>0.01027</td>
<td>0.1213</td>
</tr>
<tr class="odd">
<td></td>
<td>ROSE Data</td>
<td>0.5319</td>
<td>&lt; 2.2e-16</td>
<td>0.4399</td>
</tr>
<tr class="even">
<td>Multinomial Logistic Regression</td>
<td>Bag of words</td>
<td>0.8155</td>
<td>&lt; 2.2e-16</td>
<td>0.7596</td>
</tr>
<tr class="odd">
<td></td>
<td>TF-IDF</td>
<td>0.8212</td>
<td>&lt; 2.2e-16</td>
<td>0.767</td>
</tr>
<tr class="even">
<td></td>
<td>ROSE</td>
<td>0.7961</td>
<td>&lt; 2.2e-16</td>
<td>0.7556</td>
</tr>
<tr class="odd">
<td>Decision Tree</td>
<td>Bag of words</td>
<td>0.9994</td>
<td>&lt; 2.2e-16</td>
<td>0.9993</td>
</tr>
<tr class="even">
<td></td>
<td>TF-IDF</td>
<td>0.9994</td>
<td>&lt; 2.2e-16</td>
<td>0.9993</td>
</tr>
<tr class="odd">
<td></td>
<td>ROSE</td>
<td>0.9994</td>
<td>&lt; 2.2e-16</td>
<td>0.9993</td>
</tr>
</tbody>
</table>
<p>From this table it can be seen that Neural Networks did not perform very well in comparison to the other models. The only neural network that did reasonably well was when it trained on the ROSE data. Although they did not perform well, the p-values do show that they are significant, however the Kappa values are very low. The Multinomial Logistic Regression models well, achieving high accuracy and Kappa values. The Decision Trees did even better! achieving nearly perfect test accuracy and a Kappa value close to 1.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this assignment we have shown how to clean text data, making it ready for predictive modelling. We then fit different models to the data. and assessed the respective performances using different metrics.</p>
<p>The data used for the assignmnet was text data from the SONA speeches over the years 1994 to 2023, with the speeches being the input variables and the president who delivered the speech as the output variable. In cleaning the data we created two datasets, using a bag-of-words model and a TF-IDF model, we then also noticed that the data was imbalanced and created a third dataset on the bag-of-words model using the ROSE method of over and undersampling.</p>
<p>The trained 3 different models on each of the datasets, namely a neural network, multinomial logistic regression and a decision tree. Of these, the decision tree performed the best. From the results it can also be seen how the neural network is heavily affected by an imbalanced dataset. The testing accuracy was able to improve the accounting for this. Another issue with the neural network was the time and hardware constraints. These take some time to run and only one on each dataset was done. The hyperparameters could be tuned to provide a better model with greater accuracy. The decision trees may also be overfitting - even though the testing accuracy is very high. These are done without any constraints or regularisation.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>Durbach, I. 2023. Data Science for Industry Notes. Statistical Sciences Department, University of Cape Town.</p>
<p>R Core Team. 2022. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>